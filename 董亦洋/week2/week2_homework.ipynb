{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1、使用sklearn数据集训练逻辑回归模型；\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.获取训练数据\n",
    "x,y = make_classification(n_features=10,n_samples=200)\n",
    "\n",
    "#分割数据\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,train_size=0.7)\n",
    "\n",
    "#权重参数\n",
    "theta = np.random.randn(1,10)\n",
    "bias = 0\n",
    "#超参数\n",
    "eta = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.前向运算，计算y_hat\n",
    "def cacl_y_hat(x,theta,bias):\n",
    "    z = np.dot(theta,x.T) + bias\n",
    "    y_hat = 1/(1+np.exp(-z))\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.loss函数\n",
    "def cacl_loss(y,y_hat):\n",
    "    e = 1e-8\n",
    "    return np.mean(-y*np.log(y_hat+e)-(1-y)*np.log(1-y_hat+e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.求偏导\n",
    "def calc_derivate(x,y,y_hat):\n",
    "    m = x.shape[0]\n",
    "    delta_theta = (1/m)*np.dot(y_hat-y,x)\n",
    "    delta_bias = np.mean(y_hat-y)\n",
    "    return delta_theta,delta_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.梯度下降函数\n",
    "def gradient_decent(x,y,theta,bias,eta,epoch=5000):\n",
    "    #更新斜率直到loss<某值or达到循环上限\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        #求y_hat\n",
    "        y_hat = cacl_y_hat(x,theta,bias)\n",
    "        #计算loss\n",
    "        loss = cacl_loss(y,y_hat)\n",
    "        delta_theta,delta_bias = calc_derivate(x,y,y_hat)\n",
    "        theta = theta - eta*delta_theta\n",
    "        bias = bias - eta*delta_bias\n",
    "        # if i%100==0:\n",
    "        #     print(f'loss:{loss},bias:{bias}')\n",
    "        if loss < 0.1  or i > epoch:\n",
    "            print(i)\n",
    "            return theta,bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5001\n",
      "[[-0.07264144  1.30668169  0.36064058 -0.31734432  0.67667208 -1.02896016\n",
      "   0.48176535 -0.2591473   0.38697743  0.05794741]] -0.13948034725900357\n"
     ]
    }
   ],
   "source": [
    "theta_after,bias_after = gradient_decent(x=x_train,y=y_train,theta=theta,bias=bias,eta=eta)\n",
    "print(theta_after,bias_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8166666666666667"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#验证梯度下降后参数的准确率\n",
    "def correct_rate(theta,bias,x_test,y_test):\n",
    "    i=0\n",
    "    correct_num=0\n",
    "    all = y_test.shape[0]\n",
    "    y_hat = cacl_y_hat(theta=theta,bias=bias,x=x_test)\n",
    "    while i<all:\n",
    "        if y_hat[0][i]>0.5:\n",
    "            temp=1\n",
    "        else:\n",
    "            temp=0\n",
    "        if temp==y_test[i]:\n",
    "            correct_num+=1\n",
    "        i+=1\n",
    "    return correct_num/all\n",
    "\n",
    "correct_rate(theta_after,bias_after,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5001\n",
      "5001\n",
      "5001\n",
      "[[ 0.76251315  3.21000876  1.44473879  1.02543385  3.87664824 -2.03126135\n",
      "   2.94107764 -1.66203646  0.50813148  0.65622465]] -0.4024172876702724\n",
      "[[ 0.38123451  1.94597697  0.65600667  0.43661396  1.82914927 -0.84331848\n",
      "   1.46886826 -0.82916861  0.41715768  0.15942113]] -0.3692861427270561\n",
      "[[-0.07264144  1.30668169  0.36064058 -0.31734432  0.67667208 -1.02896016\n",
      "   0.48176535 -0.2591473   0.38697743  0.05794741]] -0.13948034725900357\n"
     ]
    }
   ],
   "source": [
    "##2、调整学习率，样本数据拆分比率，观察训练结果；\n",
    "#学习率eta=0.1、0.01、0.001，可见学习率越大所需循环次数越少\n",
    "theta_after_1,bias_after_1 = gradient_decent(x=x_train,y=y_train,theta=theta,bias=bias,eta=0.1)\n",
    "theta_after_2,bias_after_2 = gradient_decent(x=x_train,y=y_train,theta=theta,bias=bias,eta=0.01)\n",
    "theta_after_3,bias_after_3 = gradient_decent(x=x_train,y=y_train,theta=theta,bias=bias,eta=0.001)\n",
    "print(theta_after_1,bias_after_1)\n",
    "print(theta_after_2,bias_after_2)\n",
    "print(theta_after_3,bias_after_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "#样本数据拆分比,，0.9、0.6、0.3、0.1\n",
    "#通常增加数据有助于提高准确率，但不是绝对的，和学习率、数据质量、训练轮次、数据分布等等有关\n",
    "x,y = make_classification(n_features=10,n_samples=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5001\n",
      "5001\n",
      "5001\n",
      "5001\n",
      "0.9准确率0.9\n",
      "0.6准确率0.9125\n",
      "0.3准确率0.8928571428571429\n",
      "0.1准确率0.8944444444444445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train_7,x_test_7,y_train_7,y_test_7 = train_test_split(x,y,train_size=0.9,shuffle=False) #分割数据\n",
    "theta_after_7,bias_after_7 = gradient_decent(x=x_train_7,y=y_train_7,theta=theta,bias=bias,eta=0.001) #计算参数\n",
    "rate_7 = correct_rate(theta_after_7,bias_after_7,x_test_7,y_test_7) #计算准确率\n",
    "\n",
    "x_train_6,x_test_6,y_train_6,y_test_6 = train_test_split(x,y,train_size=0.6,shuffle=False)\n",
    "theta_after_6,bias_after_6 = gradient_decent(x=x_train_6,y=y_train_6,theta=theta,bias=bias,eta=0.001)\n",
    "rate_6 = correct_rate(theta_after_6,bias_after_6,x_test_6,y_test_6)\n",
    "\n",
    "x_train_5,x_test_5,y_train_5,y_test_5 = train_test_split(x,y,train_size=0.3,shuffle=False)\n",
    "theta_after_5,bias_after_5 = gradient_decent(x=x_train_5,y=y_train_5,theta=theta,bias=bias,eta=0.001)\n",
    "rate_5 = correct_rate(theta_after_5,bias_after_5,x_test_5,y_test_5)\n",
    "\n",
    "x_train_4,x_test_4,y_train_4,y_test_4 = train_test_split(x,y,train_size=0.1,shuffle=False)\n",
    "theta_after_4,bias_after_4 = gradient_decent(x=x_train_4,y=y_train_4,theta=theta,bias=bias,eta=0.001)\n",
    "rate_4 = correct_rate(theta_after_4,bias_after_4,x_test_4,y_test_4)\n",
    "\n",
    "print(f'0.9准确率{rate_7}\\n0.6准确率{rate_6}\\n0.3准确率{rate_5}\\n0.1准确率{rate_4}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
