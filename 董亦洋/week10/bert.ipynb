{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":318737,"sourceType":"datasetVersion","datasetId":134082},{"sourceId":11933630,"sourceType":"datasetVersion","datasetId":7502701}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:16:26.517163Z","iopub.execute_input":"2025-05-24T15:16:26.517418Z","iopub.status.idle":"2025-05-24T15:16:26.818369Z","shell.execute_reply.started":"2025-05-24T15:16:26.517390Z","shell.execute_reply":"2025-05-24T15:16:26.817365Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/jd-comment-data-csv/jd_comment_data.csv\n/kaggle/input/jd_comment_with_label/jd_comment_data.xlsx\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### 利用huggingface中预训练模型，实现文本分类模型定制和微调\n\n1. 加载预训练模型定制输出端任务\n2. 原始数据进行清洗转换\n   - 清理停用词或非法字符\n3. 构建Dataset和DataLoader\n   - DataLoader的collate_fn参数，在回调函数中使用tokenizer转换模型输入数据\n5. 创建模型，损失函数、优化器\n6. 训练模型\n7. 观察损失调参迭代\n8. 模型保存","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:16:26.819949Z","iopub.execute_input":"2025-05-24T15:16:26.820277Z","iopub.status.idle":"2025-05-24T15:16:31.482652Z","shell.execute_reply.started":"2025-05-24T15:16:26.820255Z","shell.execute_reply":"2025-05-24T15:16:31.481640Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#数据预处理，只保留写了评价的数据\nimport csv\n\n# 用户评论数据集\nds_comments = []\n\n# 1. Read the CSV file\nwith open('/kaggle/input/jd-comment-data-csv/jd_comment_data.csv', 'r') as file:\n    reader = csv.DictReader(file)\n    for row in reader:\n        vote = int(row['评分（总分5分）(score)'])\n        content = row['评价内容(content)']\n        if content != '此用户未填写评价内容':\n            ds_comments.append([content, vote])  # 1 for positive, 0 for negative\n\nlen(ds_comments)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:16:31.484154Z","iopub.execute_input":"2025-05-24T15:16:31.484788Z","iopub.status.idle":"2025-05-24T15:16:32.195427Z","shell.execute_reply.started":"2025-05-24T15:16:31.484749Z","shell.execute_reply":"2025-05-24T15:16:32.194591Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"44422"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# 加载词典创建分词器\nfrom transformers import AutoTokenizer\ntokenizer=  AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:16:32.196727Z","iopub.execute_input":"2025-05-24T15:16:32.197030Z","iopub.status.idle":"2025-05-24T15:16:41.585820Z","shell.execute_reply.started":"2025-05-24T15:16:32.197003Z","shell.execute_reply":"2025-05-24T15:16:41.584941Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cf02a54025b4e90a7de31fc35d49dc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d25a2afb1984df4adf1d18481aa8319"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f3bf4bb34ad4704bace0d526109d080"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6dbe9743be94145b9483a71999e7cba"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ndef build_collate(tokenizer):\n    def collate_fn(batch):\n        # 文本分类语料：输入语句，类别标签\n        sentents,labels = zip(*batch)\n    \n        # tokenizer转换\n        model_inputs = tokenizer(sentents, return_tensors='pt', padding = True,  truncation = True)\n        labels = torch.tensor(labels)\n\n        return model_inputs, labels\n    return collate_fn\n\n# DataLoader\ndl = DataLoader(ds_comments, batch_size=20, shuffle=True, collate_fn=build_collate(tokenizer))\n\n\n\n# 使用预训练bert模型时，学习率不能太大!!! 推荐1e-4或1e-5 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:16:41.586671Z","iopub.execute_input":"2025-05-24T15:16:41.587047Z","iopub.status.idle":"2025-05-24T15:16:41.592271Z","shell.execute_reply.started":"2025-05-24T15:16:41.587026Z","shell.execute_reply":"2025-05-24T15:16:41.591491Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 定制模型输出\nfrom transformers import AutoModelForSequenceClassification, AutoModelForMaskedLM\n\n# 完成文本分类任务(5个类别)\nmodel = AutoModelForSequenceClassification.from_pretrained('google-bert/bert-base-chinese', num_labels=5)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:16:41.592947Z","iopub.execute_input":"2025-05-24T15:16:41.593256Z","iopub.status.idle":"2025-05-24T15:16:59.066521Z","shell.execute_reply.started":"2025-05-24T15:16:41.593227Z","shell.execute_reply":"2025-05-24T15:16:59.065790Z"}},"outputs":[{"name":"stderr","text":"2025-05-24 15:16:44.257418: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748099804.473901      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748099804.538894      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"992418bc5799466685f029dd4172fcf4"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=5, bias=True)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# 优化器、损失\noptimizer = optim.Adam(model.parameters(), lr=1e-5)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\n\n# 训练\n\nfor epoch in range(1):\n    model.train()\n    tpbar = tqdm(dl)\n    for model_inputs, labels in tpbar:\n        model_inputs, labels = model_inputs.to(device), labels.to(device)\n        # 前向传播\n        logits = model(model_inputs.input_ids).logits\n        #print(logits,labels)\n        # 计算损失\n        loss = criterion(logits.view(-1, 5), labels.view(-1)-1)\n\n        # 反向传播\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        tpbar.set_description(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n\ntorch.save(model.state_dict(), '/kaggle/working/bert_classification_JD_Comments.bin')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:16:59.068306Z","iopub.execute_input":"2025-05-24T15:16:59.068856Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/2222 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\nEpoch 1, Loss: 0.1481:  28%|██▊       | 614/2222 [04:12<08:55,  3.00it/s]","output_type":"stream"}],"execution_count":null}]}
