{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:56:05.948820Z","iopub.execute_input":"2025-05-29T12:56:05.949211Z","iopub.status.idle":"2025-05-29T12:56:05.955892Z","shell.execute_reply.started":"2025-05-29T12:56:05.949185Z","shell.execute_reply":"2025-05-29T12:56:05.954866Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"1⃣️【第十一周作业】\n\n1. 参考课堂案例，使用指定的数据集，编写代码实现ner模型训练和推流。\n   https://huggingface.co/datasets/doushabao4766/msra_ner_k_V3\n2. 完成预测结果的实体抽取。\n   输入：“双方确定了今后发展中美关系的指导方针。”\n   输出：[{\"entity\":\"ORG\",\"content\":\"中\"},{\"entity\":\"ORG\",\"content\":\"美\"}]\n3. 整理Dataset、Trainer、TrainingArgument、DataCollator、Evaluate 知识点，总结文档`","metadata":{}},{"cell_type":"code","source":"!pip install seqeval evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:56:05.957441Z","iopub.execute_input":"2025-05-29T12:56:05.957875Z","iopub.status.idle":"2025-05-29T12:56:10.222400Z","shell.execute_reply.started":"2025-05-29T12:56:05.957841Z","shell.execute_reply":"2025-05-29T12:56:10.221183Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->seqeval) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.14.0->seqeval) (2024.2.0)\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\nfrom transformers import TrainingArguments, Trainer\nimport torch\nimport evaluate  # pip install evaluate\nimport seqeval   # pip install seqeval\nfrom datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:56:10.224716Z","iopub.execute_input":"2025-05-29T12:56:10.225751Z","iopub.status.idle":"2025-05-29T12:56:10.232188Z","shell.execute_reply.started":"2025-05-29T12:56:10.225700Z","shell.execute_reply":"2025-05-29T12:56:10.230916Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"#  从预训练模型google-bert/bert-base-chinese中加载AutoModelForTokenClassification模型，并设置标签数量为7\n#  AutoModelForTokenClassification是transformers库中用于命名实体识别（NER）的预训练模型，它基于BERT模型进行微调，可以识别文本中的命名实体。\n#  AutoTokenizer是transformers库中用于文本分词的类，它可以将文本转换为模型可以理解的token索引。\n#  AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', num_labels=7)\n#  从预训练模型google-bert/bert-base-chinese中加载AutoModelForTokenClassification模型，并设置标签数量为7。\nmodel = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', num_labels=7)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:56:10.233261Z","iopub.execute_input":"2025-05-29T12:56:10.233558Z","iopub.status.idle":"2025-05-29T12:56:10.544021Z","shell.execute_reply.started":"2025-05-29T12:56:10.233538Z","shell.execute_reply":"2025-05-29T12:56:10.542960Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"#  从预训练模型google-bert/bert-base-chinese中加载tokenizer，用于将文本转换为模型可以理解的token索引。\n\ntokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T14:34:07.039176Z","iopub.execute_input":"2025-05-29T14:34:07.039514Z","iopub.status.idle":"2025-05-29T14:34:07.208609Z","shell.execute_reply.started":"2025-05-29T14:34:07.039491Z","shell.execute_reply":"2025-05-29T14:34:07.207574Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:56:10.545906Z","iopub.execute_input":"2025-05-29T12:56:10.546220Z","iopub.status.idle":"2025-05-29T12:56:10.554870Z","shell.execute_reply.started":"2025-05-29T12:56:10.546199Z","shell.execute_reply":"2025-05-29T12:56:10.553756Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=7, bias=True)\n)"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"# 模型测试\nmessage= \"命名实体识别\"\n\nlabel = torch.tensor([0,1,0,2,5,4,3,6])\n\n# 使用tokenizer对输入文本进行编码，并返回PyTorch张量\nmodel_input = tokenizer([message], return_tensors='pt')\n\nprint(model_input)\n\nresult = model(**model_input, labels=label)\n\nprint(f'result.loss={result.loss}')\nprint(f'result.logits={result.logits}')\nprint(f'result.logits.shape={result.logits.shape}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T14:34:15.133710Z","iopub.execute_input":"2025-05-29T14:34:15.134074Z","iopub.status.idle":"2025-05-29T14:34:15.324340Z","shell.execute_reply.started":"2025-05-29T14:34:15.134050Z","shell.execute_reply":"2025-05-29T14:34:15.322703Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[ 101, 1462, 1399, 2141,  860, 6399, 1166,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\nresult.loss=2.128631353378296\nresult.logits=tensor([[[ 0.4046, -0.4229,  0.7871,  0.2858, -0.5718,  0.2152,  0.3736],\n         [ 0.0599, -0.6414,  0.2595,  0.1233, -0.0846,  0.2574,  0.4637],\n         [ 0.7908,  0.0814,  0.8804,  0.4730, -0.3888,  0.2857, -0.5470],\n         [ 0.3755, -0.0178,  0.1393,  0.3736, -0.4602, -0.0115, -0.6532],\n         [ 0.8548,  0.3367,  0.6789,  0.5918, -0.2406,  0.1948, -0.4337],\n         [ 0.3224,  0.0476,  1.0836, -0.0728, -0.3192,  0.2274, -0.7668],\n         [ 0.3873,  0.3639,  0.9823,  0.0547, -0.4981,  0.2312, -0.3353],\n         [ 0.6891,  0.0187,  0.4609,  0.4001, -0.5813, -0.0659, -0.3121]]],\n       grad_fn=<ViewBackward0>)\nresult.logits.shape=torch.Size([1, 8, 7])\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"# huggingface数据及加载\nds = load_dataset(\"doushabao4766/msra_ner_k_V3\")\n# ds = load_dataset('nlhappy/CLUE-NER')\nds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T14:34:19.468158Z","iopub.execute_input":"2025-05-29T14:34:19.468488Z","iopub.status.idle":"2025-05-29T14:34:20.109754Z","shell.execute_reply.started":"2025-05-29T14:34:19.468467Z","shell.execute_reply":"2025-05-29T14:34:20.108693Z"}},"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge'],\n        num_rows: 45001\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge'],\n        num_rows: 3443\n    })\n})"},"metadata":{}}],"execution_count":75},{"cell_type":"code","source":"# 查看数据集\ntrain_data = ds['train']\n\n# for row in train_data:\n#     print(row)\n#     print(len(row['tokens']))\n#     print(len(row['ner_tags']))\n#     break\n\nfor row in train_data:\n    print(row)\n    print((row['tokens']))\n    print(len(row['ner_tags']))\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T14:34:27.424411Z","iopub.execute_input":"2025-05-29T14:34:27.424727Z","iopub.status.idle":"2025-05-29T14:34:27.433285Z","shell.execute_reply.started":"2025-05-29T14:34:27.424705Z","shell.execute_reply":"2025-05-29T14:34:27.431975Z"}},"outputs":[{"name":"stdout","text":"{'id': '0', 'tokens': ['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'knowledge': ''}\n['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！']\n50\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"#  \n# 数据集预处理确定label与数值类型之间的映射（map）\nentities = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n           'company', 'scene', 'book', 'organization', 'government'})\n\ntags = ['O']\n \nfor entity in entities[1:]:\n    tags.append('B-'+ entity.upper())\n    tags.append('I-'+ entity.upper())\n\nentity_index = {entity:i for i, entity in enumerate(entities)}\nprint(entity_index)\nprint(tags, len(tags))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T14:50:03.664045Z","iopub.execute_input":"2025-05-29T14:50:03.664449Z","iopub.status.idle":"2025-05-29T14:50:03.672970Z","shell.execute_reply.started":"2025-05-29T14:50:03.664426Z","shell.execute_reply":"2025-05-29T14:50:03.671540Z"}},"outputs":[{"name":"stdout","text":"{'O': 0, 'company': 1, 'address': 2, 'scene': 3, 'name': 4, 'book': 5, 'government': 6, 'position': 7, 'game': 8, 'movie': 9, 'organization': 10}\n['O', 'B-COMPANY', 'I-COMPANY', 'B-ADDRESS', 'I-ADDRESS', 'B-SCENE', 'I-SCENE', 'B-NAME', 'I-NAME', 'B-BOOK', 'I-BOOK', 'B-GOVERNMENT', 'I-GOVERNMENT', 'B-POSITION', 'I-POSITION', 'B-GAME', 'I-GAME', 'B-MOVIE', 'I-MOVIE', 'B-ORGANIZATION', 'I-ORGANIZATION'] 21\n","output_type":"stream"}],"execution_count":97},{"cell_type":"code","source":"# 原始文本转换模型需要token_idx,生成和token_idx对齐label\ndef data_input_proc(item):\n    # 输入文本转换模型输入token索引\n    all_texts = [''.join(tokens) for tokens in item['tokens']]  # 每个 token 列表拼接成字符串\n    input_data = tokenizer(all_texts, truncation=True, add_special_tokens=False, max_length=512)\n    adjust_labels = []  # 所有修正后label索引列表\n    # 上一步骤生成ner_tag中索引和token对齐\n    for k in range(len(input_data['input_ids'])):\n        # 每条记录token对应word_ids\n        word_ids = input_data.word_ids(k)\n        # 批次ner_tag长度和token长度对齐\n        tags = item['ent_tag'][k]\n        \n        adjusted_label_ids = []\n        i, prev_wid = -1,-1\n        for wid in word_ids:\n            if (wid != prev_wid):   #  word_ids [1,1,1,2,3,4,5] -> [0,1,2,3,4,5,6]\n                i += 1 # token对应检索位置+1\n                prev_wid = wid\n            adjusted_label_ids.append(tags[i])\n        adjust_labels.append(adjusted_label_ids)                \n    # 修正后label添加到input_data\n    input_data['labels'] = adjust_labels\n    return input_data\n    \n# 正确使用 map：启用 batched 并设置 batch_size\nds_map = ds.map(input_data_proc_batched, batched=True, batch_size=1000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T14:42:39.324577Z","iopub.execute_input":"2025-05-29T14:42:39.324864Z","iopub.status.idle":"2025-05-29T14:43:10.194232Z","shell.execute_reply.started":"2025-05-29T14:42:39.324846Z","shell.execute_reply":"2025-05-29T14:43:10.193367Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9efdad5116834dbe95911267000b0e2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3443 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85487e128e8f4d1295c38361eac3a5ea"}},"metadata":{}}],"execution_count":85},{"cell_type":"code","source":"ds_map","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T14:43:11.638859Z","iopub.execute_input":"2025-05-29T14:43:11.639236Z","iopub.status.idle":"2025-05-29T14:43:11.646521Z","shell.execute_reply.started":"2025-05-29T14:43:11.639212Z","shell.execute_reply":"2025-05-29T14:43:11.645501Z"}},"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 45001\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 3443\n    })\n})"},"metadata":{}}],"execution_count":86},{"cell_type":"code","source":"# 记录转换为pytorch\nds_map.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T14:43:37.638766Z","iopub.execute_input":"2025-05-29T14:43:37.639218Z","iopub.status.idle":"2025-05-29T14:43:37.646683Z","shell.execute_reply.started":"2025-05-29T14:43:37.639190Z","shell.execute_reply":"2025-05-29T14:43:37.645678Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir=\"ner_train\",  # 模型训练工作目录（tensorboard，临时模型存盘文件，日志）\n    num_train_epochs = 3,    # 训练 epoch\n    save_safetensors=False,  # 设置False保存文件可以通过torch.load加载\n    per_device_train_batch_size=32,  # 训练批次\n    per_device_eval_batch_size=32,\n    report_to='tensorboard',  # 训练输出记录\n    eval_strategy=\"epoch\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T14:50:14.718042Z","iopub.execute_input":"2025-05-29T14:50:14.718418Z","iopub.status.idle":"2025-05-29T14:50:14.725275Z","shell.execute_reply.started":"2025-05-29T14:50:14.718397Z","shell.execute_reply":"2025-05-29T14:50:14.724206Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"id2lbl = {i:tag for i, tag in enumerate(tags)}\nlbl2id = {tag:i for i, tag in enumerate(tags)}\n\nmodel = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', \n                                                        num_labels=21,\n                                                        id2label=id2lbl,\n                                                        label2id=lbl2id)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T14:50:31.874798Z","iopub.execute_input":"2025-05-29T14:50:31.875160Z","iopub.status.idle":"2025-05-29T14:50:32.116336Z","shell.execute_reply.started":"2025-05-29T14:50:31.875137Z","shell.execute_reply":"2025-05-29T14:50:32.115258Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":100,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=21, bias=True)\n)"},"metadata":{}}],"execution_count":100},{"cell_type":"code","source":"# metric 方法\ndef compute_metric(result):\n    # result 是一个tuple (predicts, labels)\n    \n    # 获取评估对象\n    seqeval = evaluate.load('seqeval')\n    predicts,labels = result\n    predicts = np.argmax(prdicts, axis=2)\n    \n    # 准备评估数据\n    predicts = [[tags[p] for p,l in zip(ps,ls) if l != -100]\n                 for ps,ls in zip(predicts,labels)]\n    labels = [[tags[l] for p,l in zip(ps,ls) if l != -100]\n                 for ps,ls in zip(predicts,labels)]\n    results = seqeval.compute(predictions=predicts, references=labels)\n\n    return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T14:50:43.288481Z","iopub.execute_input":"2025-05-29T14:50:43.288775Z","iopub.status.idle":"2025-05-29T14:50:43.296755Z","shell.execute_reply.started":"2025-05-29T14:50:43.288755Z","shell.execute_reply":"2025-05-29T14:50:43.295625Z"}},"outputs":[],"execution_count":101},{"cell_type":"code","source":"data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True)\n\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=ds_map['train'],\n    eval_dataset=ds_map['test'],\n    data_collator=data_collator,\n    compute_metrics=compute_metric\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T14:51:09.677999Z","iopub.execute_input":"2025-05-29T14:51:09.678329Z","iopub.status.idle":"2025-05-29T14:51:09.694186Z","shell.execute_reply.started":"2025-05-29T14:51:09.678308Z","shell.execute_reply":"2025-05-29T14:51:09.693269Z"}},"outputs":[],"execution_count":104},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T14:51:16.233662Z","iopub.execute_input":"2025-05-29T14:51:16.234061Z","execution_failed":"2025-05-29T14:53:37.135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result = trainer.predict(ds_map['validation'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(ds1['test'][10]['tokens'])\nprint(ds2['test'][10]['labels'])\nprint(result.label_ids[10])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" [tags[p] for p,l in zip(result.label_ids[10],ds_map['test'][10]['labels'])]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"[tags[l] for p,l in zip(result.label_ids[10],ds2['test'][10]['labels'])]","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}